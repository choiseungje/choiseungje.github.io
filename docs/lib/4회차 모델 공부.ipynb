{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.324227Z",
     "start_time": "2024-08-06T13:37:09.275286Z"
    }
   },
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.332700Z",
     "start_time": "2024-08-06T13:37:09.327233Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "b5ff437e08151914",
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.476970Z",
     "start_time": "2024-08-06T13:37:09.338709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([transforms.Resize((64, 64)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "dataset = ImageFolder(\n",
    "    root=\"C:/Users/com/Downloads/datasets/train\",\n",
    "    transform=transform,\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ],
   "id": "7dd11907e2a5b41d",
   "outputs": [],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.492358Z",
     "start_time": "2024-08-06T13:37:09.478985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(Generator, self).__init__()\n",
    "      \n",
    "       # 생성자를 구성하는 층 정의\n",
    "       self.gen = nn.Sequential(\n",
    "           nn.ConvTranspose2d(100, 512, kernel_size=4, bias=False),\n",
    "           nn.BatchNorm2d(512),\n",
    "           nn.ReLU(),\n",
    "\n",
    "           nn.ConvTranspose2d(512, 256, kernel_size=4, \n",
    "                              stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(256),\n",
    "           nn.ReLU(),\n",
    "\n",
    "           nn.ConvTranspose2d(256, 128, kernel_size=4, \n",
    "                              stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(128),\n",
    "           nn.ReLU(),\n",
    "\n",
    "           nn.ConvTranspose2d(128, 64, kernel_size=4, \n",
    "                              stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(64),\n",
    "           nn.ReLU(),\n",
    "\n",
    "           nn.ConvTranspose2d(64, 3, kernel_size=4, \n",
    "                              stride=2, padding=1, bias=False),\n",
    "           nn.Tanh()\n",
    "       )\n",
    "\n",
    "   def forward(self, x):\n",
    "       return self.gen(x)"
   ],
   "id": "61f0f002634127a8",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.505610Z",
     "start_time": "2024-08-06T13:37:09.495366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(Discriminator, self).__init__()\n",
    "      \n",
    "       # 감별자를 구성하는 층의 정의\n",
    "       self.disc = nn.Sequential(\n",
    "           nn.Conv2d(3, 64, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(64),\n",
    "           nn.LeakyReLU(0.2),\n",
    "\n",
    "           nn.Conv2d(64, 128, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(128),\n",
    "           nn.LeakyReLU(0.2),\n",
    "\n",
    "           nn.Conv2d(128, 256, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(256),\n",
    "           nn.LeakyReLU(0.2),\n",
    "\n",
    "           nn.Conv2d(256, 512, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=False),\n",
    "           nn.BatchNorm2d(512),\n",
    "           nn.LeakyReLU(0.2),\n",
    "\n",
    "           nn.Conv2d(512, 1, kernel_size=4),\n",
    "           nn.Sigmoid()\n",
    "       )\n",
    "\n",
    "   def forward(self, x):\n",
    "       return self.disc(x)"
   ],
   "id": "fd48f88076661880",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.517455Z",
     "start_time": "2024-08-06T13:37:09.507620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def weights_init(m):\n",
    "    # 층의 종류 추출\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        # 합성곱층 초기화\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        # 배치정규화층 초기화\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ],
   "id": "cf87108b84956c24",
   "outputs": [],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:09.694772Z",
     "start_time": "2024-08-06T13:37:09.519462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 생성자 정의\n",
    "G = Generator().to(device)\n",
    "# ❶ 생성자 가중치 초기화\n",
    "G.apply(weights_init)\n",
    "\n",
    "# 감별자 정의\n",
    "D = Discriminator().to(device)\n",
    "# ❷ 감별자 가중치 초기화\n",
    "D.apply(weights_init)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "D_optim = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_optim = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ],
   "id": "2409ee91c6c37c8d",
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:17.408958Z",
     "start_time": "2024-08-06T13:37:09.695782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epochs in range(1):\n",
    "    iterator = tqdm.tqdm(enumerate(loader, 0), total=len(loader))\n",
    "    for i, data in iterator:\n",
    "        D_optim.zero_grad()\n",
    "\n",
    "        real = D(data[0].to(device))\n",
    "        label = torch.ones_like(data[1], dtype=torch.float32).to(device)\n",
    "        Dloss_real = nn.BCEWithLogitsLoss()(torch.squeeze(real), label)\n",
    "        Dloss_real.backward(retain_graph=True)  # retain_graph=True\n",
    "        noise = torch.randn(label.shape[0], 100, 1, 1, device=device)\n",
    "        fake = G(noise)\n",
    "        output = D(fake.detach())\n",
    "        label_fake = torch.zeros_like(data[1], dtype=torch.float32).to(device)\n",
    "        Dloss_fake = nn.BCEWithLogitsLoss()(torch.squeeze(output), label_fake)\n",
    "        Dloss_fake.backward()\n",
    "\n",
    "        D_optim.step()\n",
    "\n",
    "        # 생성자 학습\n",
    "        G_optim.zero_grad()\n",
    "        output = D(fake)\n",
    "        Gloss = nn.BCEWithLogitsLoss()(torch.squeeze(output), label)\n",
    "        Gloss.backward()\n",
    "        G_optim.step()\n",
    "\n",
    "        iterator.set_description(f\"epoch:{epochs} iteration:{i} D_loss:{Dloss_real + Dloss_fake} G_loss:{Gloss}\")\n",
    "\n",
    "    # 모델 저장\n",
    "    torch.save(G.state_dict(), \"Generator.pth\")\n",
    "    torch.save(D.state_dict(), \"Discriminator.pth\")"
   ],
   "id": "e0cb4fda8f52eae0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0 iteration:2 D_loss:1.3324689865112305 G_loss:0.6917083859443665:   2%|▏         | 3/155 [00:07<06:14,  2.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[212], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# 생성자 학습\u001B[39;00m\n\u001B[0;32m     20\u001B[0m G_optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 21\u001B[0m output \u001B[38;5;241m=\u001B[39m D(fake)\n\u001B[0;32m     22\u001B[0m Gloss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mBCEWithLogitsLoss()(torch\u001B[38;5;241m.\u001B[39msqueeze(output), label)\n\u001B[0;32m     23\u001B[0m Gloss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[209], line 32\u001B[0m, in \u001B[0;36mDiscriminator.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisc(x)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T13:37:17.412076Z",
     "start_time": "2024-08-06T13:37:17.411056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "   G.load_state_dict(\n",
    "       torch.load(\"./Generator.pth\", map_location=device))\n",
    "   # 특징 공간 상의 랜덤한 하나의 점을 지정\n",
    "   feature_vector = torch.randn(1, 100, 1, 1).to(device)\n",
    "   # 이미지 생성\n",
    "   pred = G(feature_vector).squeeze()\n",
    "   pred = pred.permute(1, 2, 0).cpu().numpy()\n",
    "   plt.imshow(pred)\n",
    "   plt.title(\"predicted image\")\n",
    "   plt.show()"
   ],
   "id": "48d24ed1cdd6c9ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "    ",
   "id": "2940e4a366b46bd1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
